name: nodecft

data:
  data_root: data/CiteSeer2
  dataset: CiteSeer2
  seed: 44

# model for data
model:
  _target_: models.nodecft.BaseGraph
  in_feat: 3703
  hidden_dim: 32
  num_class: 2
  aggr: 'gat'
  layers: 3

# optimizer for training task model
optimizer:
  _target_: torch.optim.SGD
  lr: 0.1
  momentum: 0.95
  weight_decay: 0.0005

# lr scheduler for training task optimizer
lr_scheduler:
  _target_: torch.optim.lr_scheduler.MultiStepLR
  milestones: [50ï¼Œ100]
  gamma: 0.2

epoch: 200
save_num_model: 25
train_layer: 'all'
# train_layer: ['conv_layers.0.att_src', 'conv_layers.0.att_dst', 'conv_layers.0.bias', 'conv_layers.0.lin.weight', 'conv_layers.1.att_src', 'conv_layers.1.att_dst', 'conv_layers.1.bias', 'conv_layers.1.lin.weight', ]
# train_layer: ['out.bias', 'out.lin.weight']

# parameter data root
param:
  data_root: param_data/CiteSeer/data.pt
  k: 50
  token_length: 128
  num_workers: 4
  num_encoder_latents: 8
  dim_ae: 64
  train_list: ['CiteSeer1', 'CiteSeer2']
  test_list: ['CiteSeer1', 'CiteSeer2', 'CiteSeer3']
  test_shape: [119782]
  test_dim: [32]


