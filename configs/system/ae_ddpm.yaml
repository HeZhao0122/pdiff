name: ae_ddpm

ae_model:
#  _target_: core.module.modules.encoder.medium
#  in_dim: 8576
#  input_noise_factor: 0.001
#  latent_noise_factor: 0.05
#2626
#    _target_: core.module.modules.odvae.medium
#    in_dim: 241874
#    latent_dim: 1544
#    kld_weight: 0.01

    _target_: core.module.modules.perceiverAE.PerceiverAutoEncoder
    dim_lm: 128
    dim_ae: 64
    depth: 4
    dim_head: 64
    num_encoder_latents: 8
    num_decoder_latents: 287
    max_seq_len: 2000
    ff_mult: 4
    encoder_only: False
    transformer_decoder: False
    l2_normalize_latents: False
#
#    _target_: core.module.modules.transformer_enc.TF
#    in_dim: 36681
#    dim_list: [64, 64, 64, 32000, 64, 64, 64, 4096, 3, 3, 3, 192]
#    latent_dim: 512
#    len_token: 16
#    input_noise_factor: 0.001
#    latent_noise_factor: 0.5
#    num_layers: 3
#    param_layer: 2

#    _target_: core.module.modules.transformer_enc.TFVae
#    in_dim: 36681
#    dim_list: [64, 64, 64, 32000, 64, 64, 64, 4096, 3, 3, 3, 192]
#    latent_dim: 512
#    len_token: 32
#    input_noise_factor: 0.001
#    latent_noise_factor: 0.5
#    num_layers: 3
#    kld_weight: 0.1

self_condition: False
p: 0.8
in_dim: 36681

model:
  arch:
    _target_: core.module.wrapper.ema.EMA
    model:
#      _target_: core.module.modules.unet.AE_small
#      in_channel: 1
#      in_dim: 512
#      cond_dim: 512
#      latent_dim: 32
      _target_: core.module.modules.unet.TF
      in_dim: 12
      cond_dim: 512
      latent_dim: 64
      ff_dim: 512
      num_layers: 3
      self_cond: False
#        _target_: core.module.modules.DiT.DiT
#        input_size: 512
#        patch_size: 4
#        in_channels: 1
#        hidden_size: 384
#        depth: 12
#        num_heads: 6
#        mlp_ratio: 4.0
#        learn_sigma: False

#     model:
#       _target_: core.module.modules.od_unet.AE_CNN_bottleneck
#       in_dim: 52


beta_schedule:
  start: 1e-4
  end: 2e-2
  schedule: linear
  n_timestep: 500

model_mean_type: eps
model_var_type: fixedlarge
loss_type: mse

train:
  split_epoch: 1200
  datasets: ['PubMed',]
  fine_tune_epoch: 2400
  generate_epoch: 200
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-2
    weight_decay: 2e-5

  ae_optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-2
    weight_decay: 2e-6

  lr_scheduler:

  trainer:
    _target_: pytorch_lightning.Trainer
    _convert_: all
    max_epochs: 10000
    check_val_every_n_epoch:
    val_check_interval : 900
    log_every_n_steps: 8
    limit_val_batches: 1
    limit_test_batches: 1
    devices:
      - ${device.id}

    enable_model_summary: false

    callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: 'best_g_acc'
      mode: 'max'
      save_top_k: 1
      save_last: true
      filename: 'ddpm-{epoch}-{best_g_acc:.4f}'

#     - _target_: pytorch_lightning.callbacks.ModelCheckpoint
# #       dirpath: ${output_dir}/${system.name}/checkpoints
#       filename: "ae-{epoch}-{loss:.4f}"
#       monitor: 'ae_loss'
#       mode: 'min'
#       save_top_k: 1
#       save_last: false
#       verbose: true

    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
#       dirpath: ${output_dir}/${system.name}/checkpoints
      filename: "ae-{epoch}-{ae_acc:.4f}"
      monitor: 'ae_acc'
      mode: 'max'
      save_top_k: 1
      save_last: false
      verbose: true

    logger:
      _target_: pytorch_lightning.loggers.TensorBoardLogger
      save_dir: ${output_dir}/${system.name}/
      name: '.'
      version: '.'